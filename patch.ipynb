{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches_2d(img,patch_shape,step=[1.0,1.0],batch_first=False):\n",
    "    patch_H, patch_W = patch_shape[0], patch_shape[1]\n",
    "    if(img.size(2)<patch_H):\n",
    "        num_padded_H_Top = (patch_H - img.size(2))//2\n",
    "        num_padded_H_Bottom = patch_H - img.size(2) - num_padded_H_Top\n",
    "        padding_H = nn.ConstantPad2d((0,0,num_padded_H_Top,num_padded_H_Bottom),0)\n",
    "        img = padding_H(img)\n",
    "    if(img.size(3)<patch_W):\n",
    "        num_padded_W_Left = (patch_W - img.size(3))//2\n",
    "        num_padded_W_Right = patch_W - img.size(3) - num_padded_W_Left\n",
    "        padding_W = nn.ConstantPad2d((num_padded_W_Left,num_padded_W_Right,0,0),0)\n",
    "        img = padding_W(img)\n",
    "    step_int = [0,0]\n",
    "    step_int[0] = int(patch_H*step[0]) if(isinstance(step[0], float)) else step[0]\n",
    "    step_int[1] = int(patch_W*step[1]) if(isinstance(step[1], float)) else step[1]\n",
    "    patches_fold_H = img.unfold(2, patch_H, step_int[0])\n",
    "    if((img.size(2) - patch_H) % step_int[0] != 0):\n",
    "        patches_fold_H = torch.cat((patches_fold_H,img[:,:,-patch_H:,].permute(0,1,3,2).unsqueeze(2)),dim=2)\n",
    "    patches_fold_HW = patches_fold_H.unfold(3, patch_W, step_int[1])   \n",
    "    if((img.size(3) - patch_W) % step_int[1] != 0):\n",
    "        patches_fold_HW = torch.cat((patches_fold_HW,patches_fold_H[:,:,:,-patch_W:,:].permute(0,1,2,4,3).unsqueeze(3)),dim=3)\n",
    "    patches = patches_fold_HW.permute(2,3,0,1,4,5)\n",
    "    patches = patches.reshape(-1,img.size(0),img.size(1),patch_H,patch_W)\n",
    "    if(batch_first):\n",
    "        patches = patches.permute(1,0,2,3,4)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-0bb04b22e6d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mimg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mimg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mimg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((478,478))\n",
    "                                       ,transforms.ToTensor()])\n",
    "\n",
    "data = ImageFolder('C:/Users/yonsei/Desktop/asd',transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False, num_workers=5, pin_memory=True)\n",
    "index = 0\n",
    "for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "    img = extract_patches_2d(inputs,[299,299],step=[0.6,0.6],batch_first=True)\n",
    "    for i in range(16):\n",
    "        index = index +1\n",
    "        for j in range(4):\n",
    "            img1 = img[i][j]\n",
    "            img1 = img1.numpy()\n",
    "            img1 = img1.transpose(1, 2, 0)\n",
    "            image = (img1 * 255).round().astype(np.uint8)\n",
    "            image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "            #cv2.imwrite('D:/capstone/split_patient/S1/data/5fold/train/benign/benign_%d_%d.jpg'%(index,j),image )\n",
    "            cv2.imwrite('C:/Users/yonsei/Desktop/asd/benign/259-2_%d.jpg'%(j+1),image)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
