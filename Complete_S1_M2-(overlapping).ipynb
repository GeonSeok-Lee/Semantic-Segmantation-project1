{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "from torchvision import transforms, models\n",
    "import pretrainedmodels\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches_2d(img,patch_shape,step=[1.0,1.0],batch_first=False):\n",
    "    patch_H, patch_W = patch_shape[0], patch_shape[1]\n",
    "    if(img.size(2)<patch_H):\n",
    "        num_padded_H_Top = (patch_H - img.size(2))//2\n",
    "        num_padded_H_Bottom = patch_H - img.size(2) - num_padded_H_Top\n",
    "        padding_H = nn.ConstantPad2d((0,0,num_padded_H_Top,num_padded_H_Bottom),0)\n",
    "        img = padding_H(img)\n",
    "    if(img.size(3)<patch_W):\n",
    "        num_padded_W_Left = (patch_W - img.size(3))//2\n",
    "        num_padded_W_Right = patch_W - img.size(3) - num_padded_W_Left\n",
    "        padding_W = nn.ConstantPad2d((num_padded_W_Left,num_padded_W_Right,0,0),0)\n",
    "        img = padding_W(img)\n",
    "    step_int = [0,0]\n",
    "    step_int[0] = int(patch_H*step[0]) if(isinstance(step[0], float)) else step[0]\n",
    "    step_int[1] = int(patch_W*step[1]) if(isinstance(step[1], float)) else step[1]\n",
    "    patches_fold_H = img.unfold(2, patch_H, step_int[0])\n",
    "    if((img.size(2) - patch_H) % step_int[0] != 0):\n",
    "        patches_fold_H = torch.cat((patches_fold_H,img[:,:,-patch_H:,].permute(0,1,3,2).unsqueeze(2)),dim=2)\n",
    "    patches_fold_HW = patches_fold_H.unfold(3, patch_W, step_int[1])   \n",
    "    if((img.size(3) - patch_W) % step_int[1] != 0):\n",
    "        patches_fold_HW = torch.cat((patches_fold_HW,patches_fold_H[:,:,:,-patch_W:,:].permute(0,1,2,4,3).unsqueeze(3)),dim=3)\n",
    "    patches = patches_fold_HW.permute(2,3,0,1,4,5)\n",
    "    patches = patches.reshape(-1,img.size(0),img.size(1),patch_H,patch_W)\n",
    "    if(batch_first):\n",
    "        patches = patches.permute(1,0,2,3,4)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#misclassified된 이미지들을 저장하는 함수\n",
    "def misimshow(inp,name,index):\n",
    "    inp = inp.transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.title('misclassified as %s' %(name))\n",
    "    #plt.savefig('D:/capstone/split_patient/S4/Output/misclassified_test/4fold/%d.jpg'%(index))\n",
    "\n",
    "#각 set의 개수는 용도로 쓰이는 함수(단, 시간이 오래걸려 사용안함)\n",
    "def num_of_label(loader):\n",
    "    global n\n",
    "    label_list = []\n",
    "    n=0\n",
    "    while n < len(loader):\n",
    "        if loader == trainloader.dataset:\n",
    "            set = 'Train set'\n",
    "            label_lists = np.array(train[n][1])\n",
    "            label_list = np.append(label_list,label_lists)\n",
    "            n+=1\n",
    "        elif loader == validloader.dataset:\n",
    "            set = 'Valid set'\n",
    "            label_lists = np.array(valid[n][1])\n",
    "            label_list = np.append(label_list,label_lists)\n",
    "            n+=1\n",
    "        elif loader == testloader.dataset:\n",
    "            set = 'Test set'\n",
    "            label_lists = np.array(test[n][1])\n",
    "            label_list = np.append(label_list,label_lists)\n",
    "            n+=1\n",
    "    unique, counts = np.unique(label_list, return_counts=True)\n",
    "    print('{} : {}'.format(set,dict(zip(unique, counts))))\n",
    "\n",
    "#결과의 accuracy나 loss변화를 보는 그래프\n",
    "def result_graph():\n",
    "    plt.figure(1)\n",
    "    \n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(range(1,len(train_accuracy)+1),train_accuracy,'b',label = 'train accuracy')\n",
    "    plt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'valid accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('acc')\n",
    "    plt.title('Acc Curve')\n",
    "    plt.legend()\n",
    "    plt.subplots_adjust(hspace=0.7)\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(range(1,len(train_losses)+1),train_losses,'b',label = 'train loss')\n",
    "    plt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'valid loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "\n",
    "#학습시 일정 주기(patience)동안 가장 낮은 로스 기준으로 더 낮은 로스가 나타나지 않으면 조기 학습종료\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        self._step = 0\n",
    "        self._loss = float('inf')\n",
    "        self.patience  = patience\n",
    "        self.verbose = verbose\n",
    " \n",
    "    def validate(self, loss):\n",
    "        if self._loss < loss:\n",
    "            self._step += 1\n",
    "            if self._step > self.patience:\n",
    "                if self.verbose:\n",
    "                    print('Training process is stopped early....')\n",
    "                return True\n",
    "        else:\n",
    "            self._step = 0\n",
    "            self._loss = loss\n",
    " \n",
    "        return False\n",
    "\n",
    "#전체적인 result값을 구하고 분석하는 VAlid와 Test시에 사용\n",
    "def confmat(loader):\n",
    "    #확인하려는 모델을 부름\n",
    "    model.load_state_dict(torch.load('s3_1fold_1reapeated.pt'))\n",
    "    model.eval()\n",
    "    volatile=True\n",
    "    running_correct = 0\n",
    "    nb_classes = 2\n",
    "    index = 0\n",
    "    #패치를 위해 인풋영상을 299*299에서 upsampling하기 위해 만듬\n",
    "    up_sample = nn.UpsamplingBilinear2d(size=(478, 478))\n",
    "    roc_max_diff, roc_target = [],[]\n",
    "\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    #테스트단에서 grad를 사용하지 않으므로\n",
    "    with torch.no_grad():\n",
    "        #data와 target을 들어온 loader에서 받음\n",
    "        for data,target in loader:\n",
    "            inputs,target = data.cpu(),target.cpu()\n",
    "            if loader == validloader:\n",
    "                if is_cuda:\n",
    "                #원본에 대한 결과값\n",
    "                    inputs_original = data.cuda()\n",
    "                inputs_original = Variable(inputs_original)\n",
    "                outputs_original = model(inputs_original)\n",
    "                outputs_original = torch.sigmoid(outputs_original)\n",
    "                \n",
    "                inputs = up_sample(inputs)\n",
    "                #인풋이미지에대해 overlapping patch로 4개의 패치를 만듬 파라메터로는 inputs가 인풋 영상이고 [299,299]는\n",
    "                #실제로 받을 패치들의 사이즈, step은 가로 세로 stride이며, batch_first란 4D에서 5D의 결과값이 나올떄 batch 차원을 제일 앞에 두는 변수\n",
    "                img = extract_patches_2d(inputs,[299,299],step=[0.6,0.6],batch_first=True)\n",
    "                length = len(target)\n",
    "                if is_cuda:\n",
    "                    img,target = img.cuda(),target.cuda()\n",
    "                img , target = Variable(img),Variable(target)\n",
    "                for i in range(length):\n",
    "                    outputs = model(img[i])\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                   \n",
    "                #원본 + 패치들의 값들을 더하기\n",
    "                    for j in range(4):\n",
    "                        outputs_original[i,0] += outputs[j,0]\n",
    "                        outputs_original[i,1] += outputs[j,1]\n",
    "                    #평균을 내줌 즉, TTA(Test time augmentation)    \n",
    "                    outputs_original[i] = outputs_original[i]/5\n",
    "                    \n",
    "                #BN과 ALM에 대한 확률중 가장 큰값을 인덱스로 택함\n",
    "                _, preds = torch.max(outputs_original, 1)\n",
    "                running_correct += preds.eq(target.data.view_as(preds)).cpu().sum() \n",
    "                \n",
    "                \n",
    "                        \n",
    "            if loader == testloader:\n",
    "                if is_cuda:\n",
    "                #원본에 대한 결과값\n",
    "                    inputs_original = data.cuda()\n",
    "                inputs_original = Variable(inputs_original)\n",
    "                outputs_original = model(inputs_original)\n",
    "                outputs_original = torch.sigmoid(outputs_original)\n",
    "                \n",
    "                inputs = up_sample(inputs)\n",
    "                #인풋이미지에 의해 패치를 만드는단 \n",
    "                img = extract_patches_2d(inputs,[299,299],step=[0.6,0.6],batch_first=True)\n",
    "                length = len(target)\n",
    "                if is_cuda:\n",
    "                    img,target = img.cuda(),target.cuda()\n",
    "                img , target = Variable(img),Variable(target)\n",
    "                for i in range(length):\n",
    "                    outputs = model(img[i])\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                   \n",
    "                #원본 + 패치들의 값들을 더하기\n",
    "                    for j in range(4):\n",
    "                        outputs_original[i,0] += outputs[j,0]\n",
    "                        outputs_original[i,1] += outputs[j,1]\n",
    "                   #원본과 패치들의 값을 다 더한후 평균 내주기     \n",
    "                    outputs_original[i] = outputs_original[i]/5\n",
    "                    #ROC curve를 그리기위해 평균낸 값에서 ALM인 부분의 확률을 쌓음\n",
    "                    roc_max_diff = np.append(roc_max_diff,outputs_original[i,1])\n",
    "                load_original_preds = outputs_original.cpu()\n",
    "                load_original_preds = load_original_preds.numpy()\n",
    "                np.savetxt(output_path + \"load_original_preds.csv\", load_original_preds)\n",
    "                _, preds = torch.max(outputs_original, 1)\n",
    "                load_preds = preds.cpu()\n",
    "                load_preds = load_preds.numpy()\n",
    "                np.savetxt(output_path + \"load_preds.csv\", load_preds)\n",
    "                preds,target,inputs = preds.cpu(),target.cpu(),inputs.cpu()\n",
    "\n",
    "                #mpreds,mtarget,minputs = preds.numpy(),target.numpy(),inputs.numpy()\n",
    "                #for m in range(len(mpreds)):\n",
    "                 #   index = index +1\n",
    "                  #  o = (mpreds[m]==mtarget[m]).astype(np.float32)\n",
    "                   # if o != 1:\n",
    "                    #    mis = mpreds[m]\n",
    "                     #   if mis != 0:\n",
    "                      #      name = 'melanoma'\n",
    "                       # else:\n",
    "                        #    name = 'benign'        \n",
    "                        #misimshow(minputs[m],name,index)\n",
    "                running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "                #ROC커브를 그리기위해 실제 정답값을 쌓음\n",
    "                roc_target = np.append(roc_target,target)\n",
    "            \n",
    "            #결과를 분석하기 위해 사이킷런 기반confusion_matrix 함수를 사용 \n",
    "            if loader == validloader:\n",
    "                for t, p in zip(target.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "            if loader == testloader:   \n",
    "                for t, p in zip(target.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "                    \n",
    "        #가지고 있는 Set에 대한 평균 정확도를 측정\n",
    "        accuracy = 100. * running_correct/len(loader.dataset)\n",
    "                    \n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            s[i][j] = confusion_matrix[i][j]\n",
    "    TN,FP,FN,TP = s[0][0],s[0][1],s[1][0],s[1][1]\n",
    "    PE = ((TP+FN)/(len(loader.dataset)))*((TP+FP)/(len(loader.dataset)))+((FP+TN)/(len(loader.dataset)))*((FN+TN)/(len(loader.dataset)))\n",
    "    print(confusion_matrix)\n",
    "    print(confusion_matrix.diag()/confusion_matrix.sum(1))\n",
    "    print('TP = {}, FP = {}, TN = {}, FN = {}'.format(TP,FP,TN,FN))\n",
    "    print('Specifity = {:.4f}, Sensitivity = {:.4f}'.format(TN/(TN+FP),TP/(TP+FN)))\n",
    "    print('F1 score = {:.4f}'.format(TP/(TP+(FN+FP)/2)))\n",
    "    if loader == testloader:\n",
    "        print('Test Acc = {:.4f}'.format(accuracy))\n",
    "        accuracy = accuracy.type(torch.FloatTensor)\n",
    "        PE = PE.type(torch.FloatTensor)\n",
    "        Kappa = (0.01*accuracy-PE)/(1.0-PE)\n",
    "        Kappa = Kappa.type(torch.FloatTensor)\n",
    "        print('cohens kappa = {:.4f}'.format(Kappa))\n",
    "        #roc커브를 그리기위해 사용하는 사이킷런기반의 roc_curve함수\n",
    "        fpr, tpr, _ = roc_curve(roc_target,roc_max_diff)\n",
    "        roc_auc = auc(fpr,tpr)\n",
    "        np.save(output_path + \"fpr.npy\", fpr)\n",
    "        np.save(output_path + \"tpr.npy\", tpr)\n",
    "        np.save(output_path + \"auc.npy\", roc_auc)\n",
    "        np.save(output_path + \"roc_target.npy\", roc_target)\n",
    "        np.save(output_path + \"roc_max_diff.npy\", roc_max_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune = False\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "if not fine_tune:\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "n_features = model.fc.in_features\n",
    "model.dropout = nn.Dropout(p=0.5)\n",
    "model.fc = nn.Linear(n_features, 2)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9,nesterov=True)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습단에서 사용하는 fit함수로 confmat과 비슷함 단 여기서는 train,과 valid만 있음 정해진 에폭마다 반복시행\n",
    "def fit(epoch,model,data_loader,phase='train',volatile=False):\n",
    "    if phase == 'train':\n",
    "        exp_lr_scheduler.step()\n",
    "        model.train()\n",
    "    if phase == 'valid':\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    up_sample = nn.UpsamplingBilinear2d(size=(478, 478))\n",
    "    for batch_idx , (data,target) in enumerate(data_loader):\n",
    "        inputs,target = data.cpu(),target.cpu()\n",
    "        if phase == 'valid':\n",
    "            with torch.no_grad():\n",
    "                if is_cuda:\n",
    "            #원본에 대한 결과값\n",
    "                    inputs_original = data.cuda()\n",
    "                inputs_original = Variable(inputs_original)\n",
    "                outputs_original = model(inputs_original)\n",
    "                outputs_original = torch.sigmoid(outputs_original)\n",
    "                inputs = up_sample(inputs)\n",
    "                img = extract_patches_2d(inputs,[299,299],step=[0.6,0.6],batch_first=True)\n",
    "                length = len(target)\n",
    "                if is_cuda:\n",
    "                    img,target = img.cuda(),target.cuda()\n",
    "                img , target = Variable(img),Variable(target)\n",
    "                for i in range(length):\n",
    "                    outputs = model(img[i])\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                   \n",
    "                #원본 + 패치들의 값들을 더하기\n",
    "                    for j in range(4):\n",
    "                        outputs_original[i,0] += outputs[j,0]\n",
    "                        outputs_original[i,1] += outputs[j,1]\n",
    "                        \n",
    "                    outputs_original[i] = outputs_original[i]/5\n",
    "                    \n",
    "                loss = criterion(outputs_original,target)  \n",
    "                running_loss += loss.data.item()\n",
    "                _, preds = torch.max(outputs_original, 1)\n",
    "                running_correct += preds.eq(target.data.view_as(preds)).cpu().sum() \n",
    "        \n",
    "        if phase == 'train':\n",
    "            if is_cuda:\n",
    "                inputs,target = data.cuda(),target.cuda()\n",
    "            inputs , target = Variable(inputs,volatile),Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "         \n",
    "            output = model(inputs)\n",
    "            loss = criterion(output,target)  \n",
    "            running_loss += loss.data.item()\n",
    "            preds = output.data.max(dim=1,keepdim=True)[1]\n",
    "            running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = 100. * running_correct/len(data_loader.dataset)\n",
    "    \n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, loss, accuracy))\n",
    "    return loss,accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SET MAKING and TRAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0/399\n",
      "----------\n",
      "train Loss: 0.3257 Acc: 77.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yonsei\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:225: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\")\n",
      "C:\\Users\\yonsei\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.0467 Acc: 53.0000\n",
      "0.04671147093176842\n",
      "\n",
      "inf\n",
      "\n",
      "0\n",
      "\n",
      "Epoch 1/399\n",
      "----------\n",
      "train Loss: 0.2475 Acc: 82.0000\n",
      "valid Loss: 0.0232 Acc: 95.0000\n",
      "0.023154059424996376\n",
      "\n",
      "0.04671147093176842\n",
      "\n",
      "0\n",
      "\n",
      "Epoch 2/399\n",
      "----------\n",
      "train Loss: 0.2174 Acc: 83.0000\n",
      "valid Loss: 0.0252 Acc: 90.0000\n",
      "0.02515811212360859\n",
      "\n",
      "0.023154059424996376\n",
      "\n",
      "0\n",
      "\n",
      "Epoch 3/399\n",
      "----------\n",
      "train Loss: 0.1902 Acc: 85.0000\n",
      "valid Loss: 0.0259 Acc: 88.0000\n",
      "0.025942481122910976\n",
      "\n",
      "0.023154059424996376\n",
      "\n",
      "1\n",
      "\n",
      "Epoch 4/399\n",
      "----------\n",
      "train Loss: 0.1900 Acc: 85.0000\n",
      "valid Loss: 0.0273 Acc: 90.0000\n",
      "0.027285075560212136\n",
      "\n",
      "0.023154059424996376\n",
      "\n",
      "2\n",
      "\n",
      "Epoch 5/399\n",
      "----------\n",
      "train Loss: 0.1805 Acc: 86.0000\n",
      "valid Loss: 0.0253 Acc: 89.0000\n",
      "0.025318210013210773\n",
      "\n",
      "0.023154059424996376\n",
      "\n",
      "3\n",
      "\n",
      "Epoch 6/399\n",
      "----------\n",
      "train Loss: 0.1612 Acc: 86.0000\n",
      "valid Loss: 0.0233 Acc: 95.0000\n",
      "0.023271507397294045\n",
      "\n",
      "0.023154059424996376\n",
      "\n",
      "4\n",
      "\n",
      "Epoch 7/399\n",
      "----------\n",
      "train Loss: 0.1988 Acc: 86.0000\n",
      "valid Loss: 0.0292 Acc: 85.0000\n",
      "0.029178364016115666\n",
      "\n",
      "0.023154059424996376\n",
      "\n",
      "5\n",
      "\n",
      "Epoch 8/399\n",
      "----------\n",
      "train Loss: 0.1520 Acc: 88.0000\n",
      "valid Loss: 0.0356 Acc: 71.0000\n",
      "0.03557792417705059\n",
      "\n",
      "0.023154059424996376\n",
      "\n",
      "6\n",
      "\n",
      "Epoch 9/399\n",
      "----------\n",
      "train Loss: 0.1712 Acc: 87.0000\n",
      "valid Loss: 0.0239 Acc: 93.0000\n",
      "0.023872817121446132\n",
      "\n",
      "0.023154059424996376\n",
      "\n",
      "7\n",
      "\n",
      "Epoch 10/399\n",
      "----------\n",
      "train Loss: 0.0847 Acc: 91.0000\n",
      "valid Loss: 0.0241 Acc: 94.0000\n",
      "0.024066526629030704\n",
      "\n",
      "0.023154059424996376\n",
      "\n",
      "8\n",
      "\n",
      "Epoch 11/399\n",
      "----------\n",
      "train Loss: 0.0618 Acc: 91.0000\n",
      "valid Loss: 0.0229 Acc: 95.0000\n",
      "0.02290655430406332\n",
      "\n",
      "0.023154059424996376\n",
      "\n",
      "9\n",
      "\n",
      "Epoch 12/399\n",
      "----------\n",
      "train Loss: 0.0648 Acc: 91.0000\n",
      "valid Loss: 0.0236 Acc: 94.0000\n",
      "0.023574554920196535\n",
      "\n",
      "0.02290655430406332\n",
      "\n",
      "0\n",
      "\n",
      "Epoch 13/399\n",
      "----------\n",
      "train Loss: 0.0600 Acc: 91.0000\n",
      "valid Loss: 0.0224 Acc: 96.0000\n",
      "0.022435448318719863\n",
      "\n",
      "0.02290655430406332\n",
      "\n",
      "1\n",
      "\n",
      "Epoch 14/399\n",
      "----------\n",
      "train Loss: 0.0514 Acc: 92.0000\n",
      "valid Loss: 0.0229 Acc: 96.0000\n",
      "0.022889368794858457\n",
      "\n",
      "0.022435448318719863\n",
      "\n",
      "0\n",
      "\n",
      "Epoch 15/399\n",
      "----------\n",
      "train Loss: 0.0567 Acc: 92.0000\n",
      "valid Loss: 0.0230 Acc: 95.0000\n",
      "0.02298952452838421\n",
      "\n",
      "0.022435448318719863\n",
      "\n",
      "1\n",
      "\n",
      "Epoch 16/399\n",
      "----------\n",
      "train Loss: 0.0552 Acc: 92.0000\n",
      "valid Loss: 0.0249 Acc: 91.0000\n",
      "0.024923339113593103\n",
      "\n",
      "0.022435448318719863\n",
      "\n",
      "2\n",
      "\n",
      "Epoch 17/399\n",
      "----------\n",
      "train Loss: 0.0494 Acc: 92.0000\n",
      "valid Loss: 0.0229 Acc: 95.0000\n",
      "0.022940154559910296\n",
      "\n",
      "0.022435448318719863\n",
      "\n",
      "3\n",
      "\n",
      "Epoch 18/399\n",
      "----------\n",
      "train Loss: 0.0466 Acc: 92.0000\n",
      "valid Loss: 0.0228 Acc: 96.0000\n",
      "0.022751726768910886\n",
      "\n",
      "0.022435448318719863\n",
      "\n",
      "4\n",
      "\n",
      "Epoch 19/399\n",
      "----------\n",
      "train Loss: 0.0400 Acc: 92.0000\n",
      "valid Loss: 0.0224 Acc: 96.0000\n",
      "0.022405745275318623\n",
      "\n",
      "0.022435448318719863\n",
      "\n",
      "5\n",
      "\n",
      "Epoch 20/399\n",
      "----------\n",
      "train Loss: 0.0433 Acc: 92.0000\n",
      "valid Loss: 0.0229 Acc: 96.0000\n",
      "0.022881068475544452\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "0\n",
      "\n",
      "Epoch 21/399\n",
      "----------\n",
      "train Loss: 0.0432 Acc: 92.0000\n",
      "valid Loss: 0.0234 Acc: 95.0000\n",
      "0.023428925126791\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "1\n",
      "\n",
      "Epoch 22/399\n",
      "----------\n",
      "train Loss: 0.0394 Acc: 93.0000\n",
      "valid Loss: 0.0231 Acc: 96.0000\n",
      "0.02306205648928881\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "2\n",
      "\n",
      "Epoch 23/399\n",
      "----------\n",
      "train Loss: 0.0397 Acc: 93.0000\n",
      "valid Loss: 0.0227 Acc: 96.0000\n",
      "0.022725941613316536\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "3\n",
      "\n",
      "Epoch 24/399\n",
      "----------\n",
      "train Loss: 0.0433 Acc: 92.0000\n",
      "valid Loss: 0.0229 Acc: 95.0000\n",
      "0.022915608435869216\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "4\n",
      "\n",
      "Epoch 25/399\n",
      "----------\n",
      "train Loss: 0.0407 Acc: 93.0000\n",
      "valid Loss: 0.0225 Acc: 96.0000\n",
      "0.02250126898288727\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "5\n",
      "\n",
      "Epoch 26/399\n",
      "----------\n",
      "train Loss: 0.0437 Acc: 92.0000\n",
      "valid Loss: 0.0227 Acc: 96.0000\n",
      "0.022679853811860086\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "6\n",
      "\n",
      "Epoch 27/399\n",
      "----------\n",
      "train Loss: 0.0426 Acc: 92.0000\n",
      "valid Loss: 0.0227 Acc: 96.0000\n",
      "0.022651635855436326\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "7\n",
      "\n",
      "Epoch 28/399\n",
      "----------\n",
      "train Loss: 0.0418 Acc: 92.0000\n",
      "valid Loss: 0.0238 Acc: 95.0000\n",
      "0.02376341838389635\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "8\n",
      "\n",
      "Epoch 29/399\n",
      "----------\n",
      "train Loss: 0.0424 Acc: 93.0000\n",
      "valid Loss: 0.0229 Acc: 96.0000\n",
      "0.022898822836577893\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "9\n",
      "\n",
      "Epoch 30/399\n",
      "----------\n",
      "train Loss: 0.0382 Acc: 93.0000\n",
      "valid Loss: 0.0232 Acc: 95.0000\n",
      "0.02323809303343296\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "10\n",
      "\n",
      "Epoch 31/399\n",
      "----------\n",
      "train Loss: 0.0390 Acc: 93.0000\n",
      "valid Loss: 0.0228 Acc: 96.0000\n",
      "0.022825304046273233\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "11\n",
      "\n",
      "Epoch 32/399\n",
      "----------\n",
      "train Loss: 0.0417 Acc: 92.0000\n",
      "valid Loss: 0.0235 Acc: 95.0000\n",
      "0.02348677534610033\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "12\n",
      "\n",
      "Epoch 33/399\n",
      "----------\n",
      "train Loss: 0.0396 Acc: 93.0000\n",
      "valid Loss: 0.0229 Acc: 96.0000\n",
      "0.022850443981587885\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "13\n",
      "\n",
      "Epoch 34/399\n",
      "----------\n",
      "train Loss: 0.0439 Acc: 92.0000\n",
      "valid Loss: 0.0229 Acc: 96.0000\n",
      "0.022916404530405998\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "14\n",
      "\n",
      "Epoch 35/399\n",
      "----------\n",
      "train Loss: 0.0374 Acc: 93.0000\n",
      "valid Loss: 0.0227 Acc: 96.0000\n",
      "0.022662884183228017\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "15\n",
      "\n",
      "Epoch 36/399\n",
      "----------\n",
      "train Loss: 0.0413 Acc: 92.0000\n",
      "valid Loss: 0.0227 Acc: 96.0000\n",
      "0.022691093385219574\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "16\n",
      "\n",
      "Epoch 37/399\n",
      "----------\n",
      "train Loss: 0.0432 Acc: 93.0000\n",
      "valid Loss: 0.0234 Acc: 96.0000\n",
      "0.02336017321795225\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "17\n",
      "\n",
      "Epoch 38/399\n",
      "----------\n",
      "train Loss: 0.0429 Acc: 93.0000\n",
      "valid Loss: 0.0236 Acc: 95.0000\n",
      "0.023571154661476613\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "18\n",
      "\n",
      "Epoch 39/399\n",
      "----------\n",
      "train Loss: 0.0393 Acc: 93.0000\n",
      "valid Loss: 0.0227 Acc: 96.0000\n",
      "0.022718888707458973\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "19\n",
      "\n",
      "Epoch 40/399\n",
      "----------\n",
      "train Loss: 0.0404 Acc: 93.0000\n",
      "valid Loss: 0.0230 Acc: 96.0000\n",
      "0.023028628900647163\n",
      "\n",
      "0.022405745275318623\n",
      "\n",
      "20\n",
      "Training process is stopped early....\n",
      "\n",
      "tensor([[77.,  3.],\n",
      "        [ 4., 76.]])\n",
      "tensor([0.9625, 0.9500])\n",
      "TP = 76.0, FP = 3.0, TN = 77.0, FN = 4.0\n",
      "Specifity = 0.9625, Sensitivity = 0.9500\n",
      "F1 score = 0.9560\n",
      "\n",
      "Avg valid Loss: 0.0230 Acc: 96.0000\n",
      "Training complete in 16m 20s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#배치사이즈\n",
    "batch_size = 16\n",
    "#에폭 정하기\n",
    "num_epochs = 400\n",
    "\n",
    "#결과 값들을 받을 주소 설정\n",
    "output_path = 'D:/capstone/split_patient/S3/Output/ROC_1/'\n",
    "#이미지들을 리사이즈하고 텐서로 바꾸며, 이미지넷에서 학습된 아키텍처를 사용하므로 노멀라이즈 해줌\n",
    "transform = transforms.Compose([transforms.Resize((299,299))\n",
    "                                       ,transforms.ToTensor()\n",
    "                                       ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "transforms = transforms.Compose([transforms.Resize((478,478))\n",
    "                                       ,transforms.ToTensor()\n",
    "                                       ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "#주소에서 트랜스폼하여 폴더를 받아오고 그뒤 데이터로더로 묶어줌\n",
    "train = ImageFolder('D:/capstone/split_patient/S3/data/1fold/train',transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=5, pin_memory=True)\n",
    "\n",
    "valid = ImageFolder('D:/capstone/split_patient/S3/data/1fold/valid',transform=transforms)\n",
    "validloader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=True, num_workers=5, pin_memory=True)\n",
    "\n",
    "test = ImageFolder('D:/capstone/split_patient/raw data/test',transform=transforms)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=5, pin_memory=True)\n",
    "\n",
    "\n",
    "#K-Fold시 매 Fold당 초기 상태로 시작하기 위해 사용 (우리의 학습에는 사용하지 않음)\n",
    "init_state = copy.deepcopy(model.state_dict())\n",
    "init_state_opt = copy.deepcopy(optimizer.state_dict())\n",
    "init_state_lr = copy.deepcopy(exp_lr_scheduler.state_dict())\n",
    "\n",
    "\n",
    "#학습 시간을 재기위해\n",
    "since = time.time()\n",
    "#avg val 로스와 정확도를 위한 변수설정\n",
    "avg_val_losses, avg_val_accuracy = [],[]\n",
    "#얼리 스탑기능을 사용하기 위해\n",
    "early_stopping = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "\n",
    "#K-Fold시 매 Fold당 초기 상태로 시작하기 위해 사용 (우리의 학습에는 사용하지 않음)\n",
    "model.load_state_dict(init_state)\n",
    "optimizer.load_state_dict(init_state_opt)\n",
    "exp_lr_scheduler.load_state_dict(init_state_lr)\n",
    "\n",
    "\n",
    "train_losses, train_accuracy = [],[]\n",
    "val_losses, val_accuracy = [],[]\n",
    "    \n",
    " \n",
    "     \n",
    "#학습시 에폭을 반복하기위해\n",
    "for epoch in range(num_epochs):\n",
    "    print()\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "    #fit에서 학습시 한 에폭의 결과값을 받음\n",
    "    epoch_loss, epoch_accuracy = fit(epoch,model,trainloader,phase='train')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,validloader,phase='valid')\n",
    "    \n",
    "    #받은 값들을 전 에폭에서의 값들에 이어 붙임\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "    \n",
    "    # 얼리스탑 기능을 사용 만약 output값이 True면 for문 break\n",
    "    if early_stopping.validate(val_epoch_loss):\n",
    "        break\n",
    "\n",
    "avg_val_losses = val_losses\n",
    "avg_val_accuracy = val_accuracy\n",
    "#정해진 이름으로 모델을 저장하는 단\n",
    "torch.save(model.state_dict(),'s2-s3_1fold.pt')\n",
    "\n",
    "print()\n",
    "#confmat에서 결과 분석\n",
    "confmat(validloader)\n",
    "result_graph()\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print()\n",
    "avg_val_accuracy, avg_val_losses = sum(avg_val_accuracy), sum(avg_val_losses)\n",
    "avg_val_accuracy, avg_val_losses = np.average(avg_val_accuracy), np.average(avg_val_losses)\n",
    "print('Avg valid Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                avg_val_losses, avg_val_accuracy))\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "time_elapsed // 60, time_elapsed % 60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "batch_size = 16\n",
    "num_epochs = 400\n",
    "\n",
    "output_path = 'D:/capstone/split_patient/S3/Output/ROC_1/'\n",
    "transform = transforms.Compose([transforms.Resize((299,299))\n",
    "                                       ,transforms.ToTensor()\n",
    "                                       ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "transforms = transforms.Compose([transforms.Resize((478,478))\n",
    "                                       ,transforms.ToTensor()\n",
    "                                       ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train = ImageFolder('D:/capstone/split_patient/S3/data/1fold/train',transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=5, pin_memory=True)\n",
    "\n",
    "valid = ImageFolder('D:/capstone/split_patient/S3/data/1fold/valid',transform=transforms)\n",
    "validloader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=True, num_workers=5, pin_memory=True)\n",
    "\n",
    "test = ImageFolder('D:/capstone/split_patient/raw data/test',transform=transforms)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=5, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[86., 14.],\n",
      "        [ 9., 91.]])\n",
      "tensor([0.8600, 0.9100])\n",
      "TP = 91.0, FP = 14.0, TN = 86.0, FN = 9.0\n",
      "Specifity = 0.8600, Sensitivity = 0.9100\n",
      "F1 score = 0.8878\n",
      "Test Acc = 88.0000\n",
      "cohens kappa = 0.7600\n"
     ]
    }
   ],
   "source": [
    "#test셋의 결과 분석\n",
    "confmat(testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
